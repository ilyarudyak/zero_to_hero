We are going to follow Andrej Karpathy course on basics of Deep Learning "Zero to Hero" - see [here](https://karpathy.ai/zero-to-hero.html).

The course contains lectures on basics of backpropagation, classical deep neural nets for NLP and  as well as a transformer (10-15 lectures, so half a course from Stanford):
- The spelled-out intro to neural networks and backpropagation: building micrograd (2-3 lectures). Technical details for backprop using a specially build python module micrograd (performs autograd).
- The spelled-out intro to language modeling: building makemore (5-6 lectures). Contain all the basics of Deep Learning for NLP models starting from character-level language model (no surprise here).
- Let's build GPT: from scratch, in code, spelled out (a gem we are waiting for, 1-2 lectures);
- Let's build the GPT Tokenizer (2-3 lectures);

